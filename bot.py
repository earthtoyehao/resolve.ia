import os
import google.generativeai as genai
from groq import Groq
from dotenv import load_dotenv

load_dotenv()

class ResolveIaBlindado:
    def __init__(self):
        # --- CONFIGURA√á√ÉO GEMINI (TITULAR) ---
        try:
            # Nota: Ajustei para 0.1 para ele ser menos criativo na Fase 1
            genai.configure(api_key=os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY"))
            self.gemini_model = genai.GenerativeModel(
                model_name=os.getenv("GEMINI_MODEL", "gemini-1.5-flash"),
                generation_config={"temperature": 0.1} 
            )
            self.gemini_ok = True
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao configurar Gemini: {e}")
            self.gemini_ok = False

        # --- CONFIGURA√á√ÉO GROQ (RESERVA DE LUXO) ---
        try:
            self.groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
            # Pega do .env ou usa o Llama 3.3 como padr√£o
            self.groq_model = os.getenv("GROQ_MODEL")
            self.groq_ok = True
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao configurar Groq: {e}")
            self.groq_ok = False

    def _buscar_rag(self, query):
        """Simula√ß√£o ou chamada real do Pinecone"""
        print(f"üîç Buscando contexto para: {query}")
        # AQUI VAI SUA L√ìGICA DE PINECONE
        # return index.query(...) 
        return f"[CONTEXTO RAG] O usu√°rio perguntou sobre: {query}. (Aqui entraria o texto do PDF)"

    def _montar_prompt(self, query, contexto, fase):
        """Constr√≥i o System Prompt adaptado para o CACD 2026"""
        
        # --- FASE 1: CLASSIFICADOR BIN√ÅRIO (Rob√¥) ---
        if fase == '1':
            return f"""
            ATUE COMO UM CLASSIFICADOR L√ìGICO DE QUEST√ïES DO CEBRASPE.
            
            --- CONTEXTO (FONTE DE VERDADE) ---
            {contexto}
            -----------------------------------
            
            INPUT DO USU√ÅRIO: "{query}"
            
            SUA TAREFA:
            1. Identifique os fatos chave (datas, nomes, conceitos).
            2. Verifique se o Contexto suporta esses fatos.
            3. Verifique se a rela√ß√£o de causa e efeito est√° correta.
            4. Procure por "pegadinhas" (ex: "apenas", "exceto", "nunca").
            
            REGRAS RIG√çDAS DE RESPOSTA (OUTPUT):
            1. Se a afirma√ß√£o for verdadeira segundo o contexto -> Responda: "CERTO"
            2. Se a afirma√ß√£o for falsa segundo o contexto -> Responda: "ERRADO"
            3. Se o contexto n√£o mencionar o assunto -> Responda: "ERRO"
            
            LISTA DE PROIBI√á√ïES (N√ÉO FA√áA ISSO):
            - N√ÉO d√™ bom dia ou sauda√ß√µes.
            - N√ÉO explique o motivo.
            - N√ÉO use pontua√ß√£o final.
            - N√ÉO complete a frase (Ex: N√£o diga "O item est√° CERTO").
            
            Sua resposta deve conter EXATAMENTE UMA PALAVRA.
            """

        # --- FASE 2: TUTOR / LEDOR (Humano Culto) ---
        else:
            return f"""
            # PERSONA
            Voc√™ √© um Tutor Especialista no CACD 2026 (Diplomacia).
            Sua resposta ser√° convertida em √°udio. Mantenha formalidade, ritmo de ditado e linguagem culta.

            --- CONTEXTO (FONTE DE VERDADE) ---
            {contexto}
            -----------------------------------

            # MODO 2: TREINO DISCURSIVO E DITADO
            - O usu√°rio pediu uma reda√ß√£o, resumo ou quest√£o.
            - SUA MISS√ÉO: Ditar um modelo de resposta (Standard Answer).
            
            LIMITES DE LINHAS (Edital 2026):
            * Reda√ß√£o (Port/Ing): 65 a 70 linhas.
            * Resumo (Port): M√°x 30 linhas.
            * Discursiva (Conte√∫do): 40 a 60 linhas.
            
            ESTILO DE FALA (LEITURA PARA DITADO):
            * Inicie com: "Aqui est√° uma sugest√£o de resposta modelo. Prepare-se para o ditado."
            * Dite o texto pausadamente.
            * VERBALIZE A PONTUA√á√ÉO (Fale "V√≠rgula", "Ponto final", "Abre aspas").

            INPUT DO USU√ÅRIO:
            {query}
            """

    def _chamar_gemini(self, prompt):
        print("ü§ñ Tentando Gemini...")
        response = self.gemini_model.generate_content(prompt)
        return response.text

    def _chamar_groq(self, prompt):
        print(f"‚ö° Acionando Backup Groq: {self.groq_model}")
        
        try:
            # L√ìGICA ESPECIAL PARA O MODELO DE RACIOC√çNIO (GPT-OSS-120B)
            # Verifica se o modelo configurado tem "oss" ou "120b" no nome
            if "oss" in self.groq_model or "120b" in self.groq_model:
                chat_completion = self.groq_client.chat.completions.create(
                    messages=[{"role": "user", "content": prompt}],
                    model=self.groq_model, # openai/gpt-oss-120b
                    
                    # Par√¢metros exclusivos deste modelo
                    reasoning_effort="medium", 
                    temperature=1.0, # Precisa ser alta para racioc√≠nio
                    max_completion_tokens=8192,
                    top_p=1,
                    stream=False,
                    stop=None
                )
            
            # L√ìGICA PADR√ÉO (Llama 3, Mixtral, etc)
            else:
                chat_completion = self.groq_client.chat.completions.create(
                    messages=[{"role": "user", "content": prompt}],
                    model=self.groq_model,
                    temperature=0.1, # Precisa ser baixa para precis√£o
                    max_completion_tokens=4096,
                    top_p=1,
                    stream=False
                )

            return chat_completion.choices[0].message.content

        except Exception as e:
            print(f"‚ùå Erro Cr√≠tico no Groq Principal ({self.groq_model}): {e}")
            
            # FALLBACK DE SEGURAN√áA: Se o 120b falhar, tenta o Llama 3 b√°sico
            try:
                print("üîÑ Tentando Fallback para Llama 3.3 Versatile...")
                fallback_resp = self.groq_client.chat.completions.create(
                    messages=[{"role": "user", "content": prompt}],
                    model="llama-3.3-70b-versatile",
                    temperature=0.3
                )
                return fallback_resp.choices[0].message.content
            except:
                return "‚ö†Ô∏è Erro Fatal: Nem Gemini nem Groq responderam."

    def processar(self, inputs):
        user_input = inputs.get('user_input')
        fase = inputs.get('fase')
        prioridade = inputs.get('prioridade', 'groq')

        # 1. Busca RAG (O segredo do sucesso est√° aqui)
        contexto = self._buscar_rag(user_input)
        if not contexto:
            return "‚ö†Ô∏è Erro: N√£o encontrei material sobre isso na base de dados.", "Sistema"

        prompt_final = self._montar_prompt(user_input, contexto, fase)

        # L√≥gica de Chamada (Simplificada para focar no parsing)
        resposta_bruta = ""
        modelo_usado = ""

        # Tenta Groq Primeiro (exemplo)
        if prioridade == 'groq' and self.groq_ok:
            try:
                resposta_bruta = self._chamar_groq(prompt_final)
                modelo_usado = "Groq ‚ö°"
            except:
                pass # Tenta o pr√≥ximo...
        
        # Se n√£o tiver resposta, tenta Gemini... (sua l√≥gica de fallback continua aqui)
        if not resposta_bruta and self.gemini_ok:
            resposta_bruta = self._chamar_gemini(prompt_final)
            modelo_usado = "Gemini üíé"

        if not resposta_bruta:
            return "Erro: IAs indispon√≠veis", "Offline"

        # --- O PULO DO GATO: LIMPEZA DA RESPOSTA (PARSING) ---
        if fase == '1':
            # Normaliza para mai√∫sculo para evitar erros de digita√ß√£o da IA
            resp_upper = resposta_bruta.upper()
            
            # Procura a palavra chave final
            if "VEREDITO: CERTO" in resp_upper or "VEREDITO:CERTO" in resp_upper:
                return "CERTO", modelo_usado
            elif "VEREDITO: ERRADO" in resp_upper or "VEREDITO:ERRADO" in resp_upper:
                return "ERRADO", modelo_usado
            elif "VEREDITO: ERRO" in resp_upper:
                return "ERRO (Conte√∫do n√£o encontrado)", modelo_usado
            else:
                # Se a IA se perdeu no formato, retorna tudo para voc√™ auditar
                # Dica: √Äs vezes √© bom ver o racioc√≠nio quando ela erra
                return f"‚ö†Ô∏è Resposta fora do padr√£o:\n{resposta_bruta}", modelo_usado
        
        else:
            # Fase 2 retorna tudo (Ditado)
            return resposta_bruta, modelo_usado